# -*- coding: utf-8 -*-
"""
Created on Mon Jan 21 10:41:17 2019

@author: miles
"""

import pip #if you need to install xgboost from local whl file via regular command line (I used conda install xgboost from conda command line)
from pandas import DataFrame, read_csv
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import xgboost as xgb 
import pandas as pd
import os
import re
from sklearn.metrics import mean_squared_error, mean_absolute_error

plt.style.use('fivethirtyeight')

#change working dir
os.chdir('C:/Users/miles/Desktop/COI_baseline')

#naming input file_name object txt file
input_file= 'LD2011_2014.txt'



### BLOCK:1 Only for preparing the test DATA fom UCI ###
#clean and rename datetime column: 'record_ts'
def create_refined_df(input_filename):
    input_file = input_filename
    df = pd.read_csv(input_file, sep = ';', low_memory = False)
    df.rename(columns={'Unnamed: 0' : 'record_ts'}, inplace = True)
    df['record_ts'] = pd.to_datetime(df['record_ts'])
    for col in df.columns[1: ]:
        df[col] = pd.to_numeric(df[col].apply(lambda x: re.sub(',', '.', str(x))))
    return df


# use function above on filename
df = create_refined_df(input_file)

#look at data
df.head(2)

### End of BLOCK:1 ###
#function for keeping only observations observations from after '2011-12-31 23:45:00' & set record_ts as index
def filter_df(input_df):
    df = input_df
    df_wdata = df.iloc[:, 0:13]
    df_filtered = df_wdata[df_wdata['record_ts'] > '2011-12-31 23:45:00']
    df_filtered.set_index('record_ts', inplace = True)
    return df_filtered


#use function above 
filtered_df = filter_df(df)


#function for creating features
def create_features_online(df_in, label=None):
    df = pd.DataFrame(df_in.copy())
    df['datetime'] = df.index
    df['year'] = df['datetime'].dt.year
    df['month'] = df['datetime'].dt.month
    df['month_2'] = df['month']*df['month']
    df['month_3'] = df['month']*df['month']*df['month']
    df['hour'] = df['datetime'].dt.hour
    df['hour_2'] = df['hour']*df['hour']
    df['hour_3'] = df['hour']*df['hour']*df['hour']
    df['dayofmonth'] = df['datetime'].dt.day
    df['dayofmonth_2'] = df['dayofmonth']*df['dayofmonth']
    df['dayofmonth_3'] = df['dayofmonth']*df['dayofmonth']*df['dayofmonth']
    df['dayofyear'] = df['datetime'].dt.dayofyear
    df['dayofyear_2'] = df['dayofyear']*df['dayofyear']
    df['dayofyear_3'] = df['dayofyear']*df['dayofyear']*df['dayofyear']
    df['minute'] = df['datetime'].dt.minute
    df['minute_2'] = df['minute']*df['minute']
    df['minute_3'] = df['minute']*df['minute']*df['minute']
    df["weekday"] = df.index.weekday
    df['weekday_2']= df['weekday']*df['weekday']
    df['weekday_3']= df['weekday']*df['weekday']*df['weekday']
    df['is_weekend'] = df.weekday.isin([5,6])*1
    df['inter_hr_dow'] = df['weekday']*df['hour']
     
    #make lag features
    shift_constant = 95
    lag_start = 1
    lag_end = 20
    for i in range(lag_start, lag_end):
        df["lag_{}".format(i)] = df[label].shift(i + shift_constant)
    rolling_window = 4      
    df['rolling_mean_hour'] = df[label].shift(shift_constant).rolling(window=rolling_window).mean()
    df['rolling_sum_mins'] = df[label].shift(shift_constant).rolling(window=rolling_window).sum()
    df['rol_max_hour'] = df[label].shift(shift_constant).rolling(window=rolling_window).max()
    df['rol_min_hour'] = df[label].shift(shift_constant).rolling(window=rolling_window).min()
    df['rol_stddev_hour'] = df[label].shift(shift_constant).rolling(window=rolling_window).std()
    cols = [0,1]
    df.drop(df.columns[cols], axis = 1, inplace = True)
    features_X = df  
    #for scaling features (not using)
    #from sklearn.preprocessing import StandardScaler
    #scaler = StandardScaler()
    if label:
        feature_y = df_in[label]
        return features_X, feature_y
    return features_X


# create training and test data for MT_004 model
y_label = 'MT_008'
structured_data = pd.DataFrame(filtered_df[y_label].copy()) #.iloc[:,[3]]
#structured_data = structured_data.loc[structured_data[y_label] != 0.0 ]
split_datetime = '2014-11-01 00:00:00'
split_datetime_heldout = '2014-12-01 00:00:00'


## History until Novemeber 2014 - used for Train
mt_train = structured_data.loc[structured_data.index < split_datetime].copy()


## November for model test 
mt_test = structured_data.loc[(structured_data.index >= split_datetime) & (structured_data.index < split_datetime_heldout) ].copy()

##create training and test data
X_train, Y_train = create_features_online(mt_train, y_label)
X_test, Y_test = create_features_online(mt_test, y_label)


#function for training model
def execute_model(X_train, Y_train, X_test, Y_test):
    reg = xgb.XGBRegressor(n_estimators = 1000, colsample_bytree = 0.92, gamma = 0.41, max_depth = 5, min_child_weight = 4, reg_lambda = 0.21, subsample = 0.90)   
    reg.fit(X_train, Y_train,
            eval_set = [(X_train, Y_train), (X_test, Y_test)],
            early_stopping_rounds = 50,
            verbose = True)
    return reg


#run training function above
reg = execute_model(X_train, Y_train, X_test, Y_test)


#get the predictions
mt_test['Predicted_MT'] = reg.predict(X_test)


#look at the first 5 predictions and realization
mt_test.head(5)


#function for calculating error rates
def mean_absolute_percentage_error(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    y_true[y_true == 0.0] = 1.0
    y_pred[y_true == 0.0] = 1.0
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100


#run function above
mean_absolute_percentage_error(y_true=mt_test['MT_008'],
                               y_pred = mt_test['Predicted_MT'])


###write out result for further testing data set
#mt_test.to_csv('res_MT008.csv')##MT_008 poly2: *7.0088, ploy2 + inter:7.1421, ploy2 + interdow:7.1246, poly3: 7.0380 ploy3 + interdow:7.0841

#mt_test.to_csv('res_MT004.csv')##MT_004 poly2: 8.9804, poly2 + inter:8.9154, ploy2 + interdow:8.7296, poly3: 8.7949 ploy3 + interdow:*8.6896

#mt_test.to_csv('res_MT006.csv')##MT_006 poly2: 8.4382, poly2 + inter:*8.4004, ploy2 + interdow:9.0781, poly3: 8.6780 ploy3 + interdow:9.1903

#mt_test.to_csv('res_MT009.csv')##MT_009 poly2: 14.7885, ploy2 + inter:15.4150, ploy2 + interdow:14.8819, poly3: 15.8902 ploy3 + interdow: *14.6514

#mt_test.to_csv('res_MT011.csv')##MT_011 poly2: 10.5270, poly2 + inter: 10.5062, ploy2 + interdow:10.4079, poly3: *10.3034 ploy3 + interdow:*10.4678

##Average MAPE poly2: 9.94858*** 
##Average MAPE poly2 + inter: 10.07582
##Average MAPE poly2 + interdow: 10.0444*
##Average MAPE poly3: 10.1409
##Average MAPE poly3 + interdow: 10.01664**



############# EXTRA STUFF NOT USING ################
    df['month_p'] = poly_features.fit_transform(df['month'])
    df['hour_p'] = poly_features.fit_transform(df['hour'])
    df['dayofmonth_p'] = poly_features.fit_transform(df['dayofmonth'])
    df['dayofyear_p'] = poly_features.fit_transform(df['dayofyear'])
    df['minute_p'] = poly_features.fit_transform(df['minute'])
    df['weekday_p'] = poly_features.fit_transform(df['weekday'])
